{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regressor, in the context of machine learning and statistical modeling, is an algorithm or model that is used for predicting continuous numerical values or variables. It is designed to learn the relationship between input features and their corresponding target values, enabling it to make predictions on new, unseen data.\n",
    "\n",
    "Regressors are commonly used for solving regression problems, where the goal is to predict a numerical value based on a set of input features. The input features can be one or more independent variables, and the target variable is the dependent variable that we want to predict.\n",
    "\n",
    "Regressors learn from the training data by fitting a mathematical function or model to the input features and their corresponding target values. The specific regression algorithm used may vary depending on the problem and the characteristics of the data. Some commonly used regressors include linear regression, decision tree regression, random forest regression, support vector regression, and neural network regression.\n",
    "\n",
    "Once trained, a regressor can be used to make predictions on new data by providing the input features as input to the model, and it will output the predicted numerical value or values.\n",
    "\n",
    "Overall, a regressor is a machine learning model or algorithm that learns the mapping between input features and continuous numerical target values, enabling it to make predictions on unseen data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A literature review is a critical and comprehensive evaluation of existing research and scholarly articles, books, and other relevant sources on a specific topic. It involves systematically reviewing, summarizing, and synthesizing the available literature to identify the current state of knowledge, key theories, concepts, methodologies, and gaps in the field.\n",
    "\n",
    "The purpose of conducting a literature review is to:\n",
    "\n",
    "1. Provide an overview of the existing knowledge and research on a particular topic.\n",
    "2. Identify gaps, inconsistencies, or controversies in the literature.\n",
    "3. Understand the historical development and evolution of the topic.\n",
    "4. Identify key theories, concepts, and frameworks relevant to the topic.\n",
    "5. Identify methodologies and research approaches used in previous studies.\n",
    "6. Identify potential research questions or areas for further investigation.\n",
    "7. Provide a theoretical and conceptual foundation for a new research study.\n",
    "8. Support or challenge existing theories or assumptions in the field.\n",
    "9. Inform the development of research hypotheses or research design.\n",
    "\n",
    "A literature review involves conducting a comprehensive search of academic databases, libraries, and other relevant sources to gather relevant scholarly articles, books, conference papers, and other publications. The collected literature is then critically analyzed, organized, and synthesized to create a coherent and logical narrative that addresses the research objectives or questions.\n",
    "\n",
    "It is an essential component of research papers, dissertations, theses, and other academic or scientific publications as it helps situate the study within the existing body of knowledge, provides context, and justifies the need for the research."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box-Cox transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Box-Cox transformation is a commonly used technique for transforming non-normal data into a normal distribution. It is useful when dealing with data that violates the assumption of normality required by certain statistical models. The Box-Cox transformation is defined by the following equation:\n",
    "\n",
    "where y is the original variable, λ is the transformation parameter, and y_transformed is the transformed variable. The parameter λ can take any real value, but commonly used values include -1 (reciprocal transformation), 0 (logarithmic transformation), and 0.5 (square root transformation).\n",
    "\n",
    "To perform the Box-Cox transformation in Python, you can use the `boxcox()` function from the `scipy.stats` module. Here's an example:\n",
    "\n",
    "```python \n",
    "from scipy import stats\n",
    "\n",
    "# Assuming your data is stored in a numpy array called 'data'\n",
    "transformed_data, lambda_value = stats.boxcox(data)\n",
    "\n",
    "# 'transformed_data' contains the transformed values\n",
    "# 'lambda_value' contains the estimated optimal lambda parameter\n",
    "```\n",
    "\n",
    "The `boxcox()` function returns two values: the transformed data and the estimated lambda value. The transformed data will be a numpy array with the same shape as the input data. The lambda value indicates the estimated optimal transformation parameter that maximizes the log-likelihood function.\n",
    "\n",
    "After applying the Box-Cox transformation, it's important to evaluate the normality of the transformed data using visual inspection or statistical tests. You can use techniques such as histograms, Q-Q plots, or the Shapiro-Wilk test to assess the normality of the transformed data.\n",
    "\n",
    "Note that the Box-Cox transformation assumes that the data values are positive. If your data includes zero or negative values, you may need to add a constant to the data before applying the transformation.\n",
    "\n",
    "Remember to adjust the code according to your specific data and requirements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green|>Based on the provided dataset, it seems that the research question could be: \"What factors influence customer satisfaction in the airline industry?\"\n",
    "</font><br>\n",
    "The dataset contains various variables that could potentially affect customer satisfaction, such as Gender, Customer Type, Age, Type of Travel, Class, Flight Distance, Inflight wifi service, Departure/Arrival time convenience, Inflight entertainment, On-board service, Leg room service, Baggage handling, Check-in service, Inflight service, Cleanliness, Departure Delay in Minutes, Arrival Delay in Minutes.\n",
    "\n",
    "To explore the research question, you can analyze the relationships between these variables and customer satisfaction. By examining the data, you can identify potential confounding variables that may influence the relationship between predictors and customer satisfaction. These confounders should be included in the model to account for their effects and obtain more accurate results.\n",
    "\n",
    "Additionally, you can perform statistical analyses, such as regression modeling or hypothesis testing, to determine the significance and strength of the relationships between the predictors and customer satisfaction. This will help you understand the relative importance of each factor and identify the key drivers of customer satisfaction in the airline industry.\n",
    "\n",
    "Remember to carefully consider the limitations of the dataset and ensure the appropriateness of the statistical methods used to address the research question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Font color = green|>To identify potential confounding variables, it is necessary to consider the relationship between the predictor variables and the outcome variable (customer satisfaction) while controlling for other variables. Confounding variables are those that are associated with both the predictor variables and the outcome variable, and if not accounted for, they can distort the relationship between the predictors and the outcome.</font>\n",
    "\n",
    "Without performing a thorough analysis of the dataset, it is challenging to definitively determine the confounding variables. However, based on the dataset features you provided, some potential confounding variables could be:\n",
    "\n",
    "1. Age: Age may be associated with both the predictor variables (e.g., Gender, Type of Travel, Class) and customer satisfaction. For example, different age groups may have varying preferences or expectations when it comes to airline experiences.\n",
    "\n",
    "2. Type of Travel: The purpose of travel (Personal or Business) may be associated with both the predictor variables (e.g., Gender, Class) and customer satisfaction. Business travelers, for instance, might have different satisfaction levels compared to personal travelers due to their specific needs and expectations.\n",
    "\n",
    "3. Class: The class of travel (Eco Plus, Business, Eco) could be a confounding variable as it may be associated with both the predictor variables (e.g., Gender, Type of Travel) and customer satisfaction. Different classes may offer distinct services or amenities that can impact satisfaction levels.\n",
    "\n",
    "It is important to note that the identification of confounding variables requires a comprehensive analysis of the data, including statistical modeling and hypothesis testing. The impact of potential confounders can vary depending on the specific research question, dataset characteristics, and analytical techniques employed. Therefore, it is advisable to conduct a rigorous analysis to identify and appropriately account for confounding variables in the research study."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overdispersion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green|>Overdispersion</font> refers to a phenomenon in statistical analysis where the observed variance of a variable is larger than what would be expected based on a given statistical model. It occurs when there is excess variability in the data that cannot be explained by the model's assumptions.\n",
    "\n",
    "In the context of regression models, overdispersion often arises when the data exhibit more variation than what is predicted by a standard model, such as the Poisson or binomial distribution. This can lead to inefficient parameter estimates, invalid hypothesis tests, and incorrect confidence intervals.\n",
    "\n",
    "To address overdispersion, several approaches can be used:\n",
    "\n",
    "1. Generalized Linear Models (GLMs): GLMs extend the standard linear regression models to handle non-normal and overdispersed data. They allow for the specification of alternative distributions and link functions that can account for the excess variability.\n",
    "\n",
    "2. Negative Binomial Regression: Negative binomial regression is a specific type of GLM that is commonly used when dealing with overdispersed count data. It relaxes the assumption of equal mean and variance in Poisson regression by introducing an additional parameter to account for the extra variability.\n",
    "\n",
    "3. Quasi-likelihood Methods: Quasi-likelihood methods provide a flexible approach to modeling overdispersed data by estimating the dispersion parameter directly. These methods do not rely on specifying a specific distributional assumption but rather allow for the estimation of the dispersion parameter from the data.\n",
    "\n",
    "4. Zero-Inflated Models: In situations where there is an excessive number of zeros in the data, zero-inflated models can be employed. These models account for both excess zeros (structural zeros) and overdispersion, providing a more accurate representation of the data generating process.\n",
    "\n",
    "It is important to assess and address overdispersion to obtain reliable and accurate statistical inferences from your data. The choice of the appropriate method depends on the specific characteristics of your data and the research question at hand."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-squared Vs Adjusted R-squred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared (R²) and adjusted R-squared (R²_adj) are both statistical measures used to evaluate the goodness of fit of a regression model. They provide information about how well the model explains the variability in the dependent variable.\n",
    "\n",
    "R-squared (R²) is a measure of the proportion of variance in the dependent variable that can be explained by the independent variables in the model. It ranges from 0 to 1, where 0 indicates that the independent variables have no explanatory power, and 1 indicates a perfect fit where all the variability in the dependent variable is explained by the independent variables. R-squared does not consider the number of predictors in the model and tends to increase as more predictors are added, even if they are not meaningful. Therefore, it can be biased in favor of models with more predictors.\n",
    "\n",
    "Adjusted R-squared (R²_adj) takes into account the number of predictors in the model. It penalizes the addition of unnecessary predictors that do not contribute significantly to the explained variance. R²_adj adjusts R-squared by subtracting the impact of the number of predictors and the sample size from R-squared. The adjusted R-squared value can be lower than R-squared if the added predictors do not improve the model's fit significantly. It provides a more conservative estimate of the model's goodness of fit and helps prevent overfitting.\n",
    "\n",
    "In summary, R-squared is a measure of the overall fit of the model, while adjusted R-squared considers the model's fit relative to the number of predictors. Adjusted R-squared is often preferred when comparing models with different numbers of predictors, as it provides a more reliable assessment of the model's performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <Font color = yellow>Decission Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.05516675515227569\n",
      "R-squared =  0.9448332448477244\n",
      "adjusted_r_squared = 0.9447719336436411\n",
      "Accuracy: 0.9448332448477244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the decision tree classifier\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "r_squared = model.score(X_test, y_test)\n",
    "print('R-squared = ',r_squared)\n",
    "\n",
    "# Get the R-squared value and the number of observations and predictors\n",
    "n = len(X_test)\n",
    "k = len(X_test.columns) - 1  # Subtract 1 to exclude the constant column\n",
    "# Calculate the adjusted R-squared\n",
    "adjusted_r_squared = 1 - ((1 - r_squared) * (n - 1) / (n - k - 1))\n",
    "print('adjusted_r_squared =',adjusted_r_squared)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <Font color = yellow>Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0369226314011294\n",
      "R-squared =  0.9630773685988706\n",
      "adjusted_r_squared = 0.9630363335410196\n",
      "Accuracy: 0.9630773685988706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "r_squared = model.score(X_test, y_test)\n",
    "print('R-squared = ',r_squared)\n",
    "\n",
    "# Get the R-squared value and the number of observations and predictors\n",
    "n = len(X_test)\n",
    "k = len(X_test.columns) - 1  # Subtract 1 to exclude the constant column\n",
    "# Calculate the adjusted R-squared\n",
    "adjusted_r_squared = 1 - ((1 - r_squared) * (n - 1) / (n - k - 1))\n",
    "print('adjusted_r_squared =',adjusted_r_squared)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green|>Collapsibility</font> has known as \"Simpson's paradox,\" refers to a phenomenon in statistics where the relationship between two variables changes when a third variable is introduced or considered. It occurs when the direction or strength of an association between variables appears to be different when analyzed separately compared to when analyzed together with other variables.\n",
    "\n",
    "In the context of logistic regression, collapsibility can be observed when the effect of a particular independent variable on the dependent variable changes when other independent variables are included in the model. This change can occur in terms of the magnitude or even the direction of the effect.\n",
    "\n",
    "<font color = green|>Collapsibility</font> can arise due to confounding variables, which are variables that are associated with both the independent and dependent variables. When these confounding variables are not accounted for in the analysis, they can influence the relationship between the independent variable and the dependent variable, leading to a potential collapse of the relationship.\n",
    "\n",
    "It is important to be aware of collapsibility and carefully consider the inclusion of relevant variables in the analysis to avoid drawing misleading conclusions. Properly accounting for confounding variables and understanding the context of the analysis can help mitigate the impact of collapsibility and provide a more accurate understanding of the relationships between variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
